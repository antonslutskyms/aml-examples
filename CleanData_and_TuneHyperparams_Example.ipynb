{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Create production machine learning pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a local source directory to hold training code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_src_dir = \"./local_src\"\n",
    "os.makedirs(train_src_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the training script to disk for use in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./local_src/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {train_src_dir}/main.py\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function of the script.\"\"\"\n",
    "\n",
    "    # input and output arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data\", type=str, help=\"path to input data\")\n",
    "    parser.add_argument(\"--test_train_ratio\", type=float, required=False, default=0.25)\n",
    "    parser.add_argument(\"--n_estimators\", required=False, default=100, type=int)\n",
    "    parser.add_argument(\"--learning_rate\", required=False, default=0.1, type=float)\n",
    "    parser.add_argument(\"--registered_model_name\", type=str, help=\"model name\")\n",
    "    args = parser.parse_args()\n",
    "   \n",
    "    # Start Logging\n",
    "    mlflow.start_run()\n",
    "\n",
    "    # enable autologging\n",
    "    mlflow.sklearn.autolog()\n",
    "\n",
    "    ###################\n",
    "    #<prepare the data>\n",
    "    ###################\n",
    "    print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n",
    "\n",
    "    print(\"input data:\", args.data)\n",
    "    \n",
    "    credit_df = pd.read_csv(args.data, header=1, index_col=0)\n",
    "\n",
    "    mlflow.log_metric(\"num_samples\", credit_df.shape[0])\n",
    "    mlflow.log_metric(\"num_features\", credit_df.shape[1] - 1)\n",
    "\n",
    "    #Split train and test datasets\n",
    "    train_df, test_df = train_test_split(\n",
    "        credit_df,\n",
    "        test_size=args.test_train_ratio,\n",
    "    )\n",
    "    ####################\n",
    "    #</prepare the data>\n",
    "    ####################\n",
    "\n",
    "    ##################\n",
    "    #<train the model>\n",
    "    ##################\n",
    "    # Extracting the label column\n",
    "    y_train = train_df.pop(\"default payment next month\")\n",
    "\n",
    "    # convert the dataframe values to array\n",
    "    X_train = train_df.values\n",
    "\n",
    "    # Extracting the label column\n",
    "    y_test = test_df.pop(\"default payment next month\")\n",
    "\n",
    "    # convert the dataframe values to array\n",
    "    X_test = test_df.values\n",
    "\n",
    "    print(f\"Training with data of shape {X_train.shape}\")\n",
    "\n",
    "    clf = GradientBoostingClassifier(\n",
    "        n_estimators=args.n_estimators, learning_rate=args.learning_rate\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    ####################\n",
    "    # Log classifier accuracy\n",
    "    ####################\n",
    "\n",
    "    accuracy = clf.score(X_test, y_test)\n",
    "    print('Accuracy of SVM classifier on test set: {:.2f}'.format(accuracy))\n",
    "    mlflow.log_metric('accuracy', float(accuracy))\n",
    "\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    ###################\n",
    "    #</train the model>\n",
    "    ###################\n",
    "\n",
    "    ##########################\n",
    "    #<save and register model>\n",
    "    ##########################\n",
    "    # Registering the model to the workspace\n",
    "    print(\"Registering the model via MLFlow\")\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=clf,\n",
    "        registered_model_name=args.registered_model_name,\n",
    "        artifact_path=args.registered_model_name,\n",
    "    )\n",
    "\n",
    "    # Saving the model to a file\n",
    "    mlflow.sklearn.save_model(\n",
    "        sk_model=clf,\n",
    "        path=os.path.join(args.registered_model_name, \"trained_model\"),\n",
    "    )\n",
    "    ###########################\n",
    "    #</save and register model>\n",
    "    ###########################\n",
    "    \n",
    "    # Stop Logging\n",
    "    mlflow.end_run()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the YAML configuration for the training component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.yml\n",
    "# <component>\n",
    "name: train_todel_to_hyper_tune\n",
    "display_name: Training Component to be Hypertuned\n",
    "# version: 1 # Not specifying a version will automatically update the version\n",
    "type: command\n",
    "inputs:\n",
    "  train_data_csv: \n",
    "    type: uri_folder\n",
    "    mode: ro_mount\n",
    "  test_data_csv: \n",
    "    type: uri_file\n",
    "    mode: ro_mount\n",
    "  learning_rate:\n",
    "    type: number     \n",
    "  registered_model_name:\n",
    "    type: string\n",
    "outputs:\n",
    "  model:\n",
    "    type: uri_folder\n",
    "code: .\n",
    "environment:\n",
    "  # for this step, we'll use an AzureML curate environment\n",
    "  azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu:1\n",
    "command: >-\n",
    "  python train.py \n",
    "  --train_data ${{inputs.train_data_csv}} \n",
    "  --test_data ${{inputs.test_data_csv}} \n",
    "  --learning_rate ${{inputs.learning_rate}}\n",
    "  --registered_model_name ${{inputs.registered_model_name}} \n",
    "  --model ${{outputs.model}}\n",
    "# </component>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the training component with Azure ML using the CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"code\": \"azureml:/subscriptions/781b03e7-6eb7-4506-bab8-cf3a0d89b1d4/resourceGroups/SandboxML/providers/Microsoft.MachineLearningServices/workspaces/quick-start-tutorial/codes/f4093bb3-4a9b-4dad-854d-3a5cfa5d660b/versions/1\",\n",
      "  \"command\": \"python train.py  --train_data ${{inputs.train_data_csv}}  --test_data ${{inputs.test_data_csv}}  --learning_rate ${{inputs.learning_rate}} --registered_model_name ${{inputs.registered_model_name}}  --model ${{outputs.model}}\",\n",
      "  \"creation_context\": {\n",
      "    \"created_at\": \"2023-11-07T15:02:10.496358+00:00\",\n",
      "    \"created_by\": \"Anton Slutsky\",\n",
      "    \"created_by_type\": \"User\",\n",
      "    \"last_modified_at\": \"2023-11-07T15:02:10.572551+00:00\",\n",
      "    \"last_modified_by\": \"Anton Slutsky\",\n",
      "    \"last_modified_by_type\": \"User\"\n",
      "  },\n",
      "  \"display_name\": \"Training Component to be Hypertuned\",\n",
      "  \"environment\": \"azureml://registries/azureml/environments/AzureML-sklearn-1.0-ubuntu20.04-py38-cpu/versions/1\",\n",
      "  \"id\": \"azureml:/subscriptions/781b03e7-6eb7-4506-bab8-cf3a0d89b1d4/resourceGroups/SandboxML/providers/Microsoft.MachineLearningServices/workspaces/quick-start-tutorial/components/train_todel_to_hyper_tune/versions/1\",\n",
      "  \"inputs\": {\n",
      "    \"learning_rate\": {\n",
      "      \"optional\": false,\n",
      "      \"type\": \"number\"\n",
      "    },\n",
      "    \"registered_model_name\": {\n",
      "      \"optional\": false,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"test_data_csv\": {\n",
      "      \"optional\": false,\n",
      "      \"type\": \"uri_file\"\n",
      "    },\n",
      "    \"train_data_csv\": {\n",
      "      \"optional\": false,\n",
      "      \"type\": \"uri_folder\"\n",
      "    }\n",
      "  },\n",
      "  \"is_deterministic\": true,\n",
      "  \"name\": \"train_todel_to_hyper_tune\",\n",
      "  \"outputs\": {\n",
      "    \"model\": {\n",
      "      \"type\": \"uri_folder\"\n",
      "    }\n",
      "  },\n",
      "  \"resourceGroup\": \"SandboxML\",\n",
      "  \"resources\": {\n",
      "    \"instance_count\": 1\n",
      "  },\n",
      "  \"type\": \"command\",\n",
      "  \"version\": \"1\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading aml-examples (0.43 MBs):   0%|          | 0/432459 [00:00<?, ?it/s]\n",
      "Uploading aml-examples (0.43 MBs):   0%|          | 2008/432459 [00:00<00:22, 18902.67it/s]\n",
      "Uploading aml-examples (0.43 MBs):  41%|####      | 176095/432459 [00:00<00:00, 999335.44it/s]\n",
      "Uploading aml-examples (0.43 MBs): 100%|##########| 432459/432459 [00:00<00:00, 1572713.99it/s]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!az ml component create -f  train.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure a command job and specify hyperparameters for sweeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'azure.ai.ml.entities._builders.command.Command'>\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input\n",
    "from azure.ai.ml.sweep import Normal, Uniform\n",
    "\n",
    "registered_model_name = \"credit_defaults_model\"\n",
    "environment_name= custom_job_env.name + \"@latest\" \n",
    "job = command(\n",
    "    inputs=dict(\n",
    "        data=Input(\n",
    "            type=\"uri_file\",\n",
    "            #path=\"azureml://subscriptions/f1ea6ed8-82f3-416d-881b-8b376218bc85/resourcegroups/rg_aml/workspaces/aml-default/datastores/workspaceblobstore/paths/LocalUpload/4b1dfc4d12429b46389cabdf25b886a2/default_of_credit_card_clients.csv\",\n",
    "            #path=\"https://azuremlexamples.blob.core.windows.net/datasets/credit_card/default_of_credit_card_clients.csv\",\n",
    "            path=\"azureml:credit_cards@latest\",\n",
    "        ),\n",
    "        test_train_ratio=0.2,\n",
    "        learning_rate=0.25,\n",
    "        registered_model_name=registered_model_name,\n",
    "    ),\n",
    "    code=\"./src/\",  # location of source code\n",
    "    command=\"python main.py --data ${{inputs.data}} --test_train_ratio ${{inputs.test_train_ratio}} --learning_rate ${{inputs.learning_rate}} --registered_model_name ${{inputs.registered_model_name}}\",\n",
    "    environment=environment_name,\n",
    "    compute=\"cpu-cluster\",\n",
    "    display_name=\"03a_train_model_credit_default_prediction\",\n",
    ")\n",
    "\n",
    "print(type(job))\n",
    "\n",
    "command_job_for_sweep = job(\n",
    "    learning_rate=Uniform(min_value=0.01, max_value=0.9)\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify an early termination policy for the hyperparameter sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.sweep import MedianStoppingPolicy\n",
    "\n",
    "# Call sweep() on your command job to sweep over your parameter expressions\n",
    "sweep_job = command_job_for_sweep.sweep(\n",
    "    compute=\"cpu-cluster\",\n",
    "    sampling_algorithm=\"random\",\n",
    "    primary_metric=\"accuracy\",\n",
    "    goal=\"Maximize\",\n",
    ")\n",
    "\n",
    "# Specify your experiment details\n",
    "sweep_job.display_name = \"credit-card-sweep-example\"\n",
    "sweep_job.experiment_name = \"credit-card-sweep-example\"\n",
    "sweep_job.description = \"Run a hyperparameter sweep job.\"\n",
    "\n",
    "# Define the limits for this sweep\n",
    "sweep_job.set_limits(max_total_trials=20, max_concurrent_trials=10, timeout=7200)\n",
    "\n",
    "# Set early stopping on this one\n",
    "sweep_job.early_termination = MedianStoppingPolicy(\n",
    "    delay_evaluation=5, evaluation_interval=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the pipeline from components\n",
    "\n",
    "Now that both your components are defined and registered, you can start implementing the pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you use *input data*, *split ratio* and *registered model name* as input variables. Then call the components and connect them via their inputs/outputs identifiers. The outputs of each step can be accessed via the `.outputs` property.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    }
   },
   "source": [
    "A YAML configuration file is used to represent the Azure Machine Learning pipeline structure as a directed acyclic graph (DAG). In the YAML configuration, we can specify the pipeline description and default resources like compute and storage.  Similar to components, pipelines can have inputs and output. You can then create multiple instances of a single pipeline with different inputs.\n",
    "\n",
    "Here, we used *input data*, *split ratio* and *registered model name* as input variables. We then call the components and connect them via their inputs/outputs identifiers. The outputs of each step can be accessed via the `.outputs` property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pipeline.yml file to describe the structure of the pipeline DAG:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pipeline definition YAML describing the DAG of jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing local_pipeline.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile local_pipeline.yml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/pipelineJob.schema.json\n",
    "type: pipeline\n",
    "\n",
    "display_name: 04a_Pipeline_DataPrep_Train_CLI_Pipeline\n",
    "description: Pipeline with 2 component jobs with data dependencies\n",
    "\n",
    "settings:\n",
    "  default_compute: azureml:cpu-cluster\n",
    "\n",
    "outputs:\n",
    "  final_model:\n",
    "    mode: upload\n",
    "\n",
    "jobs:\n",
    "  component_data_prep:\n",
    "    type: command\n",
    "    inputs:\n",
    "      - name: Dataset\n",
    "        type: DataFrameDirectory\n",
    "        description: Dataset to be cleaned\n",
    "      - name: Columns to be cleaned\n",
    "        type: ColumnPicker\n",
    "        description: Columns for missing values clean operation\n",
    "        columnPickerFor: Dataset\n",
    "      - name: Minimum missing value ratio\n",
    "        type: Float\n",
    "        default: 0.0\n",
    "        description: Clean only column with missing value ratio above specified value, out\n",
    "          of set of all selected columns\n",
    "        min: 0.0\n",
    "        max: 1.0\n",
    "      - name: Maximum missing value ratio\n",
    "        type: Float\n",
    "        default: 1.0\n",
    "        description: Clean only columns with missing value ratio below specified value,\n",
    "          out of set of all selected columns\n",
    "        min: 0.0\n",
    "        max: 1.0\n",
    "      - name: Cleaning mode\n",
    "        type: Mode\n",
    "        default: Custom substitution value\n",
    "        description: Algorithm to clean missing values\n",
    "        options:\n",
    "        - Custom substitution value:\n",
    "          - name: Replacement value\n",
    "            type: String\n",
    "            default: '0'\n",
    "            optional: true\n",
    "            description: Type the value that takes the place of missing values\n",
    "          - name: Generate missing value indicator column\n",
    "            type: Boolean\n",
    "            default: false\n",
    "            description: Generate a column that indicates which rows were cleaned\n",
    "        - Replace with mean:\n",
    "          - name: Cols with all missing values\n",
    "            type: Mode\n",
    "            default: Remove\n",
    "            description: Cols with all missing values\n",
    "            options:\n",
    "            - Propagate\n",
    "            - Remove\n",
    "          - name: Generate missing value indicator column\n",
    "            type: Boolean\n",
    "            default: false\n",
    "            description: Generate a column that indicates which rows were cleaned\n",
    "        - Replace with median:\n",
    "          - name: Cols with all missing values\n",
    "            type: Mode\n",
    "            default: Remove\n",
    "            description: Cols with all missing values\n",
    "            options:\n",
    "            - Propagate\n",
    "            - Remove\n",
    "          - name: Generate missing value indicator column\n",
    "            type: Boolean\n",
    "            default: false\n",
    "            description: Generate a column that indicates which rows were cleaned\n",
    "        - Replace with mode:\n",
    "          - name: Cols with all missing values\n",
    "            type: Mode\n",
    "            default: Remove\n",
    "            description: Cols with all missing values\n",
    "            options:\n",
    "            - Propagate\n",
    "            - Remove\n",
    "          - name: Generate missing value indicator column\n",
    "            type: Boolean\n",
    "            default: false\n",
    "            description: Generate a column that indicates which rows were cleaned\n",
    "        - Remove entire row\n",
    "        - Remove entire column\n",
    "      outputs:\n",
    "      - name: Cleaned dataset\n",
    "        type: DataFrameDirectory\n",
    "        description: Cleaned dataset\n",
    "      - name: Cleaning transformation\n",
    "        type: TransformationDirectory\n",
    "        description: Transformation to be passed to Apply Transformation module to clean\n",
    "          new data\n",
    "    code: data_prep\n",
    "    environment: \n",
    "      image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:latest\n",
    "      conda_file: conda.yml\n",
    "    compute: azureml:cpu-cluster\n",
    "    command: >-\n",
    "      python data_prep.py \n",
    "      --data ${{inputs.data}} \n",
    "      --test_train_ratio ${{inputs.test_train_ratio}} \n",
    "      --train_data_csv ${{outputs.train_data_csv}}\n",
    "      --test_data_csv ${{outputs.test_data_csv}}\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the pipeline job to Azure ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Met error <class 'AttributeError'>:'str' object has no attribute 'items'\n",
      "Please check log by running the command with '--debug' for more details.\n"
     ]
    }
   ],
   "source": [
    "!az ml job create --file .\\local_pipeline.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use your pipeline definition to instantiate a pipeline with your dataset, split rate of choice and the name you picked for your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can track the progress of your pipeline, by using the link generated in the previous cell. When you first select this link, you may see that the pipeline is still running. Once it's complete, you can examine each component's results.\n",
    "\n",
    "Double-click the **Train Credit Defaults Model** component. \n",
    "\n",
    "There are two important results you'll want to see about training:\n",
    "\n",
    "* View your logs:\n",
    "    1. Select the **Outputs+logs** tab.\n",
    "    1. Open the folders to `user_logs` > `std_log.txt`\n",
    "    This section shows the script run stdout.\n",
    "    ![Screenshot of std_log.txt.](media/user-logs.jpg)\n",
    "\n",
    "* View your metrics: Select the **Metrics** tab.  This section shows different logged metrics. In this example. mlflow `autologging`, has automatically logged the training metrics.\n",
    "    \n",
    "    ![Screenshot shows logged metrics.txt.](./media/metrics.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model as an online endpoint\n",
    "To learn how to deploy your model to an online endpoint, see [Deploy a model as an online endpoint tutorial](https://learn.microsoft.com/en-us/azure/machine-learning/tutorial-deploy-model).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Learn how to [Schedule machine learning pipeline jobs](https://learn.microsoft.com/azure/machine-learning/how-to-schedule-pipeline-job)"
   ]
  }
 ],
 "metadata": {
  "categories": [
   "SDK v2",
   "tutorials",
   "get-started-notebooks"
  ],
  "description": {
   "description": "Create production ML pipelines with Python SDK v2 in a Jupyter notebook"
  },
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "aigbb-aml-bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
